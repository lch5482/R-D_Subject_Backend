import requests
from bs4 import BeautifulSoup
import os
import time
from datetime import datetime
from urllib.parse import urljoin
import re

class MSSCrawler:
    """ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ ê³µê³  í¬ë¡¤ëŸ¬ (í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)"""
    
    def __init__(self, download_dir='downloads'):
        self.base_url = "https://www.mss.go.kr"
        self.list_url = "https://www.mss.go.kr/site/smba/ex/bbs/List.do"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://www.mss.go.kr/'
        }
        self.session = requests.Session()
        self.download_dir = download_dir
        
        if not os.path.exists(download_dir):
            os.makedirs(download_dir)
    
    def get_page(self, page_num=1, year='2025', month='00'):
        """íŠ¹ì • í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        params = {
            'cbIdx': '310',
            'year': year,
            'month': month,
            'pageIndex': page_num
        }
        
        try:
            response = self.session.get(
                self.list_url,
                params=params,
                headers=self.headers,
                timeout=15
            )
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"í˜ì´ì§€ {page_num} ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def get_total_pages(self, html):
        """ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # "í˜„ì¬ í˜ì´ì§€ : 1/184" íŒ¨í„´ ì°¾ê¸°
        list_info = soup.select_one('.list_info')
        if list_info:
            match = re.search(r'(\d+)/(\d+)', list_info.text)
            if match:
                return int(match.group(2))
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ì—ì„œ ì°¾ê¸°
        pagination = soup.select('.pagination a.page-link')
        if pagination:
            last_page = 1
            for link in pagination:
                text = link.text.strip()
                if text.isdigit():
                    last_page = max(last_page, int(text))
            return last_page
        
        return 1
    
    def get_notice_list_with_files(self, html):
        """HTMLì—ì„œ ê³µê³  ëª©ë¡ê³¼ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, 'html.parser')
        notices = []
        
        rows = soup.select('table tbody tr')
        
        for row in rows:
            try:
                notice = {}
                
                # ë²ˆí˜¸
                num_td = row.select_one('td:nth-child(1)')
                notice['number'] = num_td.text.strip() if num_td else ''
                
                # ì œëª©
                title_td = row.select_one('td.subject')
                if title_td:
                    title_link = title_td.select_one('a.pc-detail')
                    if title_link:
                        notice['title'] = title_link.text.strip()
                
                # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
                notice['attachments'] = []
                file_td = row.select_one('td.attached-files')
                if file_td:
                    file_spans = file_td.select('span.single-file')
                    for file_span in file_spans:
                        file_url = file_span.get('data-href')
                        if file_url:
                            if not file_url.startswith('http'):
                                file_url = self.base_url + file_url
                            
                            file_name = file_url.split('streFileNm=')[-1] if 'streFileNm=' in file_url else f'file_{len(notice["attachments"])}'
                            
                            notice['attachments'].append({
                                'name': file_name,
                                'url': file_url
                            })
                
                # ë‚ ì§œ
                date_td = row.select_one('td:nth-child(4)')
                notice['date'] = date_td.text.strip() if date_td else ''
                
                # ì¡°íšŒìˆ˜
                view_td = row.select_one('td:nth-child(5)')
                notice['views'] = view_td.text.strip() if view_td else '0'
                
                notices.append(notice)
                
            except Exception as e:
                print(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        return notices
    
    def download_file(self, file_url, file_name, notice_folder):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            safe_filename = re.sub(r'[<>:"/\\|?*]', '_', file_name).strip()
            
            if '.' in safe_filename:
                base_name, ext = os.path.splitext(safe_filename)
            else:
                if '.hwp' in file_url:
                    ext = '.hwp'
                elif '.pdf' in file_url:
                    ext = '.pdf'
                elif '.xlsx' in file_url:
                    ext = '.xlsx'
                elif '.docx' in file_url:
                    ext = '.docx'
                elif '.hwpx' in file_url:
                    ext = '.hwpx'
                else:
                    ext = ''
                base_name = safe_filename
                safe_filename = base_name + ext
            
            file_path = os.path.join(notice_folder, safe_filename)
            
            if os.path.exists(file_path):
                print(f"    â­ ì´ë¯¸ ì¡´ì¬: {safe_filename}")
                return file_path
            
            print(f"    â¬‡ ë‹¤ìš´ë¡œë“œ ì¤‘: {safe_filename}")
            
            response = self.session.get(file_url, headers=self.headers, timeout=30, stream=True)
            
            content_disposition = response.headers.get('Content-Disposition', '')
            if content_disposition:
                if "filename*=" in content_disposition:
                    match = re.search(r"filename\*=UTF-8''(.+)", content_disposition)
                    if match:
                        from urllib.parse import unquote
                        real_filename = unquote(match.group(1))
                        safe_filename = re.sub(r'[<>:"/\\|?*]', '_', real_filename).strip()
                        file_path = os.path.join(notice_folder, safe_filename)
                elif 'filename=' in content_disposition:
                    match = re.search(r'filename[^;=\n]*=["\']?([^"\';]+)', content_disposition)
                    if match:
                        from urllib.parse import unquote
                        real_filename = unquote(match.group(1))
                        safe_filename = re.sub(r'[<>:"/\\|?*]', '_', real_filename).strip()
                        file_path = os.path.join(notice_folder, safe_filename)
            
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            file_size = os.path.getsize(file_path)
            print(f"    âœ“ ì™„ë£Œ: {safe_filename} ({file_size:,} bytes)")
            
            return file_path
            
        except Exception as e:
            print(f"    âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def crawl_and_download(self, max_pages=3, max_items_per_page=10, year='2025', month='00'):
        """í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ë° ë‹¤ìš´ë¡œë“œ"""
        print("=" * 70)
        print("ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ ê³µê³  í¬ë¡¤ëŸ¬ ì‹œì‘ (í˜ì´ì§€ë„¤ì´ì…˜)")
        print("=" * 70)
        
        # ì²« í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        html = self.get_page(1, year, month)
        if not html:
            print("ì²« í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
        total_pages = self.get_total_pages(html)
        print(f"\nì „ì²´ í˜ì´ì§€ ìˆ˜: {total_pages}")
        
        if max_pages:
            total_pages = min(total_pages, max_pages)
            print(f"í¬ë¡¤ë§í•  í˜ì´ì§€: {total_pages}í˜ì´ì§€\n")
        
        all_notices = []
        
        # ê° í˜ì´ì§€ í¬ë¡¤ë§
        for page_num in range(1, total_pages + 1):
            print(f"\n{'='*70}")
            print(f"[í˜ì´ì§€ {page_num}/{total_pages}] í¬ë¡¤ë§ ì¤‘...")
            print('='*70)
            
            if page_num == 1:
                page_html = html
            else:
                page_html = self.get_page(page_num, year, month)
                time.sleep(1)
            
            if not page_html:
                print(f"âš  í˜ì´ì§€ {page_num} ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
                continue
            
            # ê³µê³  ëª©ë¡ ì¶”ì¶œ
            notices = self.get_notice_list_with_files(page_html)
            
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            if max_items_per_page:
                notices = notices[:max_items_per_page]
            
            print(f"\në°œê²¬ëœ ê³µê³ : {len(notices)}ê°œ\n")
            
            # ê° ê³µê³  ì²˜ë¦¬
            for idx, notice in enumerate(notices, 1):
                print(f"[{idx}/{len(notices)}] {notice['title']}")
                print(f"  ë‚ ì§œ: {notice['date']} | ì¡°íšŒ: {notice['views']}")
                print(f"  ì²¨ë¶€íŒŒì¼: {len(notice['attachments'])}ê°œ")
                
                if notice['attachments']:
                    # í´ë” ìƒì„±
                    safe_title = re.sub(r'[<>:"/\\|?*]', '_', notice['title'])[:50]
                    notice_folder = os.path.join(
                        self.download_dir,
                        f"{notice['number']}_{safe_title}"
                    )
                    
                    if not os.path.exists(notice_folder):
                        os.makedirs(notice_folder)
                    
                    print(f"  ğŸ’¾ ì €ì¥: {notice_folder}")
                    
                    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    for attachment in notice['attachments']:
                        self.download_file(
                            attachment['url'],
                            attachment['name'],
                            notice_folder
                        )
                        time.sleep(0.5)
                else:
                    print(f"  â„¹ ì²¨ë¶€íŒŒì¼ ì—†ìŒ")
                
                print()
            
            all_notices.extend(notices)
        
        print("\n" + "=" * 70)
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ì´ {len(all_notices)}ê°œ ê³µê³  ì²˜ë¦¬")
        print(f"ì²¨ë¶€íŒŒì¼ ìˆëŠ” ê³µê³ : {len([n for n in all_notices if n['attachments']])}ê°œ")
        print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜: {os.path.abspath(self.download_dir)}")
        print("=" * 70)
        
        return all_notices


if __name__ == "__main__":
    crawler = MSSCrawler(download_dir='mss_downloads')
    
    # ì˜µì…˜ 1: ìµœê·¼ 3í˜ì´ì§€, ê° í˜ì´ì§€ë‹¹ 5ê°œ ê³µê³ ë§Œ
    results = crawler.crawl_and_download(
        max_pages=5,
        max_items_per_page=10,
        year='2025',
        month='00'
    )
    
    # ì˜µì…˜ 2: ì „ì²´ í˜ì´ì§€, ê° í˜ì´ì§€ ì „ì²´ ê³µê³ 
    # results = crawler.crawl_and_download(
    #     max_pages=None,
    #     max_items_per_page=None,
    #     year='2025',
    #     month='00'
    # )